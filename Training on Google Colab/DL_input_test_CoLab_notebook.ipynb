{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_input_test_CoLab_notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akmazad/deepBrain/blob/master/Training%20on%20Google%20Colab/DL_input_test_CoLab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF5nbjNPqDKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bQOwm_eUKo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as tdata\n",
        "import torch.nn.functional as tfunc\n",
        "import torch.optim as topti\n",
        "import logging\n",
        "import os\n",
        "import torch.cuda\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.tensor\n",
        "\n",
        "class config():\n",
        "  def __init__(self):\n",
        "    self.name = \"deepbrainStaticConvnet\"\n",
        "    self.DataDir = \"gdrive/My Drive/chr_5_files_TF_signal_filtered/\"\n",
        "#     self.TrainingDataFile = \"tempTrain5_trainingData_TF_filtered_chr5_value.npy\"\n",
        "#     self.TrainingLabelFile = \"tempTrain5_trainingData_TF_filtered_chr5_label_all.npy\"\n",
        "#     self.TestingDataFile = \"tempTrain5_validationData_TF_filtered_chr5_value.npy\"\n",
        "#     self.TestingLabelFile = \"tempTrain5_validationData_TF_filtered_chr5_label_all.npy\"\n",
        "\n",
        "    self.TrainingDataFile = \"H3K27ac_rnaSeq.Pos.tfSpecific_trainingData_TF_filtered_chr5_value.npy\"\n",
        "    self.TrainingLabelFile = \"H3K27ac_rnaSeq.Pos.tfSpecific_trainingData_TF_filtered_chr5_label_all.npy\"\n",
        "    self.TestingDataFile = \"H3K27ac_rnaSeq.Pos.tfSpecific_validationData_TF_filtered_chr5_value.npy\"\n",
        "    self.TestingLabelFile = \"H3K27ac_rnaSeq.Pos.tfSpecific_validationData_TF_filtered_chr5_label_all.npy\"\n",
        "    \n",
        "    self.w_lr = 1e-2\n",
        "    self.w_lr_min = 8e-7\n",
        "    self.w_momentum = 0.9\n",
        "    self.w_weight_decay = 5e-7\n",
        "    self.w_grad_clip = 5\n",
        "    self.print_freq = 10\n",
        "    self.init_channels = 16\n",
        "    self.layers = 8\n",
        "    self.BATCH_SIZE = 128\n",
        "    self.seed = 0\n",
        "    self.workers = 4 \n",
        "    self.alpha_lr = 3e-4\n",
        "    self.alpha_weight_decay = 1e-4\n",
        "    self.world_size = -1\n",
        "    self.rank = 0\n",
        "    self.dist_url = 'env://'\n",
        "    self.dist_backend = 'nccl'\n",
        "    self.gpu = None\n",
        "    self.multiprocessing_distributed = True\n",
        "    self.nEpochs = 15\n",
        "    self.start_epoch = 0\n",
        "\n",
        "    # architecture-related parameters\n",
        "    # 3 conv-layers, 1 fully connected layers (See DeepSEA paper)\n",
        "    self.CONV1_INPUT_CHANNELS = 4\n",
        "    self.CONV1_OUTPUT_CHANNELS = 320\n",
        "    self.CONV2_OUTPUT_CHANNELS = 480\n",
        "    self.CONV3_OUTPUT_CHANNELS = 960\n",
        "    self.KERNEL_SIZE = 8\n",
        "    self.POOLING_TH = 4\n",
        "    self.DROPOUT_l1 = 0.2\n",
        "    self.DROPOUT_l2 = 0.2\n",
        "    self.DROPOUT_l3 = 0.5\n",
        "    self.NUM_OUTPUTS = 131\n",
        "    self.SEQ_LEN = 1000\n",
        "\n",
        "    \n",
        "best_acc1 = 0\n",
        "\n",
        "# model zone\n",
        "class BuildModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(BuildModel, self).__init__()\n",
        "\n",
        "        # Create and initialise weights and biases for the layers.\n",
        "        # regularization parameters could be introduced later (see deepsea/deepsea_model.py)\n",
        "        self.conv_layer1 = nn.Conv1d(args.CONV1_INPUT_CHANNELS, args.CONV1_OUTPUT_CHANNELS, args.KERNEL_SIZE)\n",
        "        self.conv_layer2 = nn.Conv1d(args.CONV1_OUTPUT_CHANNELS, args.CONV2_OUTPUT_CHANNELS, args.KERNEL_SIZE)\n",
        "        self.conv_layer3 = nn.Conv1d(args.CONV2_OUTPUT_CHANNELS, args.CONV3_OUTPUT_CHANNELS, args.KERNEL_SIZE)\n",
        "\n",
        "        nChannel = math.floor((args.SEQ_LEN - (args.KERNEL_SIZE - 1)) / args.POOLING_TH)\n",
        "        nChannel = math.floor((nChannel - (args.KERNEL_SIZE - 1)) / args.POOLING_TH)\n",
        "        nChannel = math.floor((nChannel - (args.KERNEL_SIZE - 1)) / args.POOLING_TH)\n",
        "        self.fc1 = nn.Linear(args.CONV3_OUTPUT_CHANNELS * nChannel, args.NUM_OUTPUTS)\n",
        "\n",
        "    def forward(self, x, args):\n",
        "        # Create the forward pass through the network.\n",
        "        x = self.conv_layer1(x)\n",
        "        x = tfunc.leaky_relu(x, 0.01)\n",
        "        x = tfunc.max_pool1d(x, args.POOLING_TH)  # downsample by half (i.e. if parameter=4, then by quarter)\n",
        "        # x = tfunc.batch_norm(x, torch.zeros(args.CONV1_OUTPUT_CHANNELS), torch.ones(args.CONV1_OUTPUT_CHANNELS))\n",
        "        x = tfunc.dropout(x, args.DROPOUT_l1)\n",
        "\n",
        "        x = self.conv_layer2(x)\n",
        "        x = tfunc.leaky_relu(x, 0.01)\n",
        "        x = tfunc.max_pool1d(x, args.POOLING_TH)\n",
        "        # x = tfunc.batch_norm(x, torch.zeros(args.CONV2_OUTPUT_CHANNELS), torch.ones(args.CONV2_OUTPUT_CHANNELS))\n",
        "        x = tfunc.dropout(x, args.DROPOUT_l2)\n",
        "\n",
        "        x = self.conv_layer3(x)\n",
        "        x = tfunc.leaky_relu(x, 0.1)\n",
        "        x = tfunc.max_pool1d(x, args.POOLING_TH)\n",
        "        # x = tfunc.batch_norm(x, torch.zeros(args.CONV3_OUTPUT_CHANNELS), torch.ones(args.CONV3_OUTPUT_CHANNELS))\n",
        "        x = tfunc.dropout(x, args.DROPOUT_l3)\n",
        "\n",
        "        # for the fully connected layer\n",
        "        x = x.view(x.shape[0], -1)  # Flatten tensor.\n",
        "        x = self.fc1(x)\n",
        "        x = tfunc.leaky_relu(x, 0.01)\n",
        "        #         x = tfunc.dropout(x, 0.2)\n",
        "        # x = tfunc.batch_norm(x, torch.zeros(K).cuda(), torch.ones(K).cuda())\n",
        "        x = torch.sigmoid(x)\n",
        "        # x = F.log_softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class DeepSEA(nn.Module):\n",
        "    def __init__(self, sequence_length, n_genomic_features):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        sequence_length : int\n",
        "        n_genomic_features : int\n",
        "        \"\"\"\n",
        "        super(DeepSEA, self).__init__()\n",
        "        conv_kernel_size = 8\n",
        "        pool_kernel_size = 4\n",
        "\n",
        "        self.conv_net = nn.Sequential(\n",
        "            nn.Conv1d(4, 320, kernel_size=conv_kernel_size),\n",
        "            # nn.ReLU(inplace=True),\n",
        "            nn.Threshold(0, 1e-06),\n",
        "\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=pool_kernel_size, stride=pool_kernel_size),\n",
        "            nn.Dropout(p=0.2),\n",
        "\n",
        "            nn.Conv1d(320, 480, kernel_size=conv_kernel_size),\n",
        "            # nn.ReLU(inplace=True),\n",
        "            nn.Threshold(0, 1e-06),\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=pool_kernel_size, stride=pool_kernel_size),\n",
        "            nn.Dropout(p=0.2),\n",
        "\n",
        "            nn.Conv1d(480, 960, kernel_size=conv_kernel_size),\n",
        "            # nn.ReLU(inplace=True),\n",
        "            nn.Threshold(0, 1e-06),\n",
        "            nn.Dropout(p=0.5))\n",
        "\n",
        "        reduce_by = conv_kernel_size - 1\n",
        "        pool_kernel_size = float(pool_kernel_size)\n",
        "        self.n_channels = int(\n",
        "            np.floor(\n",
        "                (np.floor(\n",
        "                    (sequence_length - reduce_by) / pool_kernel_size)\n",
        "                 - reduce_by) / pool_kernel_size)\n",
        "            - reduce_by)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(960 * self.n_channels, n_genomic_features),\n",
        "            # nn.ReLU(inplace=True),\n",
        "            nn.Threshold(0, 1e-06),\n",
        "            nn.Linear(n_genomic_features, n_genomic_features),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward propagation of a batch.\n",
        "        \"\"\"\n",
        "        out = self.conv_net(x)\n",
        "        reshape_out = out.view(out.size(0), 960 * self.n_channels)\n",
        "        predict = self.classifier(reshape_out)\n",
        "        return predict\n",
        "\n",
        "\n",
        "class DeeperDeepSEA(nn.Module):\n",
        "    \"\"\"\n",
        "    A deeper DeepSEA model architecture.\n",
        "    Parameters\n",
        "    ----------\n",
        "    sequence_length : int\n",
        "        The length of the sequences on which the model trains and and makes\n",
        "        predictions.\n",
        "    n_targets : int\n",
        "        The number of targets (classes) to predict.\n",
        "    Attributes\n",
        "    ----------\n",
        "    conv_net : torch.nn.Sequential\n",
        "        The convolutional neural network component of the model.\n",
        "    classifier : torch.nn.Sequential\n",
        "        The linear classifier and sigmoid transformation components of the\n",
        "        model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sequence_length, n_targets):\n",
        "        super(DeeperDeepSEA, self).__init__()\n",
        "        conv_kernel_size = 8\n",
        "        pool_kernel_size = 4\n",
        "\n",
        "        self.conv_net = nn.Sequential(\n",
        "            nn.Conv1d(4, 320, kernel_size=conv_kernel_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(320, 320, kernel_size=conv_kernel_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=pool_kernel_size, stride=pool_kernel_size),\n",
        "            nn.BatchNorm1d(320),\n",
        "\n",
        "            nn.Conv1d(320, 480, kernel_size=conv_kernel_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(480, 480, kernel_size=conv_kernel_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(\n",
        "                kernel_size=pool_kernel_size, stride=pool_kernel_size),\n",
        "            nn.BatchNorm1d(480),\n",
        "            nn.Dropout(p=0.2),\n",
        "\n",
        "            nn.Conv1d(480, 960, kernel_size=conv_kernel_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(960, 960, kernel_size=conv_kernel_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(960),\n",
        "            nn.Dropout(p=0.2))\n",
        "\n",
        "        reduce_by = 2 * (conv_kernel_size - 1)\n",
        "        pool_kernel_size = float(pool_kernel_size)\n",
        "        self._n_channels = int(\n",
        "            np.floor(\n",
        "                (np.floor(\n",
        "                    (sequence_length - reduce_by) / pool_kernel_size)\n",
        "                 - reduce_by) / pool_kernel_size)\n",
        "            - reduce_by)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(960 * self._n_channels, n_targets),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(n_targets),\n",
        "            nn.Linear(n_targets, n_targets),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation of a batch.\n",
        "        \"\"\"\n",
        "        out = self.conv_net(x)\n",
        "        reshape_out = out.view(out.size(0), 960 * self._n_channels)\n",
        "        predict = self.classifier(reshape_out)\n",
        "        return predict\n",
        "\n",
        "class ReCodeAlphabet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ReCodeAlphabet, self).__init__()\n",
        "        #\n",
        "    def forward(self, input):\n",
        "        # Swap ACGT to AGCT\n",
        "        # array has shape (N, 4, 1, 1000)\n",
        "        # pytorch doesn't support full indexing at the moment, at some point this should work: [:,:,torch.LongTensor([0,2,1,3])]\n",
        "        input_reordered = [input[:,i,...] for i in [0,2,1,3]]\n",
        "        input = torch.stack(input_reordered, dim=1)\n",
        "        # slightly faster but ugly:\n",
        "        #input = edit_tensor_in_numpy(input, lambda x: x[:,[0,2,1,3], ...])\n",
        "        return input\n",
        "\n",
        "\n",
        "class LambdaBase(nn.Sequential):\n",
        "    def __init__(self, fn, *args):\n",
        "        super(LambdaBase, self).__init__(*args)\n",
        "        self.lambda_func = fn\n",
        "        #\n",
        "    def forward_prepare(self, input):\n",
        "        output = []\n",
        "        for module in self._modules.values():\n",
        "            output.append(module(input))\n",
        "        return output if output else input\n",
        "\n",
        "\n",
        "class Lambda(LambdaBase):\n",
        "    def forward(self, input):\n",
        "        return self.lambda_func(self.forward_prepare(input))\n",
        "\n",
        "\n",
        "def get_model(load_weights = True):\n",
        "    deepsea_cpu = nn.Sequential( # Sequential,\n",
        "        nn.Conv2d(4,320,(1, 8),(1, 1)),\n",
        "        nn.Threshold(0, 1e-06),\n",
        "        nn.MaxPool2d((1, 4),(1, 4)),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Conv2d(320,480,(1, 8),(1, 1)),\n",
        "        nn.Threshold(0, 1e-06),\n",
        "        nn.MaxPool2d((1, 4),(1, 4)),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Conv2d(480,960,(1, 8),(1, 1)),\n",
        "        nn.Threshold(0, 1e-06),\n",
        "        nn.Dropout(0.5),\n",
        "        Lambda(lambda x: x.view(x.size(0),-1)), # Reshape,\n",
        "        nn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(50880,925)), # Linear,\n",
        "        nn.Threshold(0, 1e-06),\n",
        "        nn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(925,919)), # Linear,\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "    if load_weights:\n",
        "        deepsea_cpu.load_state_dict(torch.load('model_files/deepsea_cpu.pth'))\n",
        "    return nn.Sequential(ReCodeAlphabet(), deepsea_cpu)\n",
        "\n",
        "\n",
        "class LoadDataset(tdata.Dataset):\n",
        "    def __init__(self, args, dataPath, dataFile, labelFile):\n",
        "        # Load data from files.\n",
        "        # self.inputs = np.memmap(dataPath + dataFile, mode=\"r\").reshape(-1, args.CONV1_INPUT_CHANNELS, args.SEQ_LEN)\n",
        "        # self.labels = np.memmap(dataPath + labelFile, mode=\"r\").reshape(-1, args.NUM_OUTPUTS)\n",
        "\n",
        "        self.inputs = np.load(dataPath + dataFile)\n",
        "        self.labels = np.load(dataPath + labelFile)\n",
        "\n",
        "        self.length = len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Return a single input/label pair from the dataset.\n",
        "        inputSample = np.array(self.inputs[index], dtype=np.float32)\n",
        "        labelSample = np.array(self.labels[index], dtype=np.float32)\n",
        "        sample = (inputSample, labelSample)\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getCustomAccuracy(predicted, target, args):\n",
        "    # predicted = torch.round(torch.sigmoid(predicted))\n",
        "    # predicted = torch.round(predicted)\n",
        "    n_digits = 3\n",
        "    _predicted = torch.round(predicted * 10 ** n_digits) / (10 ** n_digits)\n",
        "    __predicted = torch.round(_predicted)\n",
        "\n",
        "    N = predicted.size(0) * args.NUM_OUTPUTS\n",
        "\n",
        "    truePred = torch.sum(torch.eq(__predicted, target)).item()\n",
        "    acc_val = truePred / N\n",
        "\n",
        "    # print(torch.sum(torch.eq(target, torch.ones(target.shape))).item())\n",
        "    # print(torch.sum(torch.eq(predicted, torch.ones(target.shape))).item())\n",
        "    return acc_val\n",
        "\n",
        "\n",
        "def getCustomAccuracy2(predicted, target, args):\n",
        "    # predicted = torch.round(torch.sigmoid(predicted))\n",
        "    # predicted = torch.round(predicted)\n",
        "    n_digits = 3    # to have something like 0.499 = 0.5\n",
        "    _predicted = torch.round(predicted * 10 ** n_digits) / (10 ** n_digits)\n",
        "    __predicted = torch.round(_predicted)\n",
        "\n",
        "\n",
        "\n",
        "    N = predicted.size(0)\n",
        "    custom_accuracy = np.zeros(args.NUM_OUTPUTS, dtype=np.float)\n",
        "    for i in range(args.NUM_OUTPUTS):\n",
        "        truePred = torch.sum(torch.eq(__predicted[:, i], target[:, i])).item()\n",
        "        custom_accuracy[i] = truePred/N\n",
        "\n",
        "    return np.median(custom_accuracy[:2]), np.median(custom_accuracy[2]), np.median(custom_accuracy[3:])\n",
        "\n",
        "\n",
        "def getCustomAccuracy3(predicted, target, args):\n",
        "    preds = []\n",
        "    targets = []\n",
        "    for i in range(10):\n",
        "        o = F.log_softmax(torch.autograd.Variable(predicted), dim=1)\n",
        "        t = torch.autograd.Variable(target)\n",
        "\n",
        "        _, pred = torch.max(o, dim=1)\n",
        "        preds.append(pred.data)\n",
        "        targets.append(t.data)\n",
        "\n",
        "    preds = torch.cat(preds)\n",
        "    targets = torch.cat(targets)\n",
        "\n",
        "\n",
        "def getAUCscore(predicted, target, args, logger):\n",
        "    # n_digits = 3\n",
        "    # _predicted = torch.round(predicted * 10**n_digits) / (10**n_digits)\n",
        "    # __predicted = torch.round(_predicted)\n",
        "\n",
        "    # _predicted = torch.round(predicted).detach()\n",
        "    __predicted = predicted.detach()\n",
        "    _target = target.detach()\n",
        "\n",
        "    aucs = np.zeros(args.NUM_OUTPUTS, dtype=np.float)\n",
        "    for i in range(args.NUM_OUTPUTS):\n",
        "        try:\n",
        "            auc = roc_auc_score(_target.cpu().numpy()[:, i], __predicted.cpu().numpy()[:, i], average='weighted')\n",
        "            aucs[i] = auc\n",
        "        except ValueError as e:\n",
        "            pass\n",
        "            # logger.info(\"NA (No positive (i.e. signal) in Test region)\")\n",
        "\n",
        "    # print('Medican AUCs: Accetylation marks: %.3f, RNA-seq: %.3f, TFs: %.3f' % (np.median(aucs[:2]), np.median(aucs[2]), np.median(aucs[3:])))\n",
        "\n",
        "    return np.median(aucs[:2]), np.median(aucs[2]), np.median(aucs[3:])\n",
        "\n",
        "\n",
        "def get_logger(file_path):\n",
        "    \"\"\" Make python logger \"\"\"\n",
        "    # [!] Since tensorboardX use default logger (e.g. logging.info()), we should use custom logger\n",
        "    logger = logging.getLogger('db2')\n",
        "    log_format = '%(asctime)s | %(message)s'\n",
        "    formatter = logging.Formatter(log_format, datefmt='%m/%d %I:%M:%S %p')\n",
        "    file_handler = logging.FileHandler(file_path)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    stream_handler.setFormatter(formatter)\n",
        "\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(stream_handler)\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "def find_perc_uncertainty(output, low, high):\n",
        "    output = output.detach().cpu().numpy()\n",
        "    return np.sum(np.logical_and(output>=low, output<=high))/output.size    # returns the proportion of tensor elements are within the range\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, args, logger, device):\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    # perc_uncertainty = 0.0\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "      \n",
        "        input = input.cuda(device, non_blocking=True)\n",
        "        target = target.cuda(device, non_blocking=True)\n",
        "        \n",
        "        # compute output\n",
        "        # output = model(input, args)\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target.squeeze(1))\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        # acc = getCustomAccuracy(output, target, args)\n",
        "        perc_uncertainty = find_perc_uncertainty(output, 0.4, 0.6)\n",
        "        custom_accuracy = getCustomAccuracy2(output, target, args)\n",
        "        # tAccuracy = getCustomAccuracy3(output, target, args)\n",
        "        aucs = getAUCscore(output, target, args, logger)\n",
        "\n",
        "        # compute gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # add l1 Sparsity\n",
        "        l1 = 0\n",
        "        for p in model.parameters():\n",
        "            l1 = l1 + p.abs().sum()\n",
        "        loss = loss + 1e-8 * l1\n",
        "\n",
        "        # do SGD step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        logger.info(\n",
        "            \"Epoch: %d, Batch: %d/%d, Loss: %.3f, perc_uncertainty: %.3f, custom[ACC:%.3f, rnaSEQ:%.3f, TFs:%.3f], roc[ACC:%.3f, rnSEQ:%.3f, TFs:%.3f]\" % (\n",
        "            epoch + 1, i, len(train_loader) - 1,\n",
        "            loss, perc_uncertainty, custom_accuracy[0], custom_accuracy[1], custom_accuracy[2], aucs[0], aucs[1], aucs[2]))\n",
        "\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, args, logger, device):\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    total_ACC, total_RNA, total_TFs = 0, 0, 0\n",
        "    perc_uncertainty = 0.0\n",
        "\n",
        "    # losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            \n",
        "            input = input.cuda(device, non_blocking=True)\n",
        "            target = target.cuda(device, non_blocking=True)\n",
        "        \n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            # losses[i] = loss\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            # acc = getCustomAccuracy(output, target, args)\n",
        "            p = find_perc_uncertainty(output, 0.4, 0.6)\n",
        "            custom_accuracy = getCustomAccuracy2(output, target, args)\n",
        "            aucs = getAUCscore(output, target, args, logger)\n",
        "\n",
        "            total_ACC += np.median(aucs[0])\n",
        "            total_RNA += np.median(aucs[1])\n",
        "            total_TFs += np.median(aucs[2])\n",
        "\n",
        "            if i % args.print_freq == 0 or i == len(val_loader) - 1:\n",
        "                # progress._print(i)\n",
        "                # logger.info(\"batch: %d, loss: %.3f; valid accuracy: custom_accuracy_metric: %.3f, ACC marks: %.3f, RNA-seq: %.3f, TFs: %.3f\" % (i+1, loss, acc, aucs[0], aucs[1], aucs[2]))\n",
        "                logger.info(\"Batch: %d/%d, Loss: %.3f, perc_uncertainty: %.3f, custom[ACC:%.3f, rnaSEQ:%.3f, TFs:%.3f], roc[ACC: %.3f, rnSEQ: %.3f, TFs:%.3f]\" % (\n",
        "                        i, len(val_loader)-1, loss, perc_uncertainty, custom_accuracy[0], custom_accuracy[1], custom_accuracy[2], aucs[0], aucs[1], aucs[2]))\n",
        "            perc_uncertainty += p\n",
        "\n",
        "        # logger.info(' * Acc@1 {top1.avg:.3f}'.format(top1=acc))\n",
        "        logger.info(\"percentage of uncertainty in validation prediction: {}\".format(perc_uncertainty / len(val_loader)))\n",
        "\n",
        "    total_ACC /= len(val_loader)\n",
        "    total_RNA /= len(val_loader)\n",
        "    total_TFs /= len(val_loader)\n",
        "\n",
        "    return np.median([total_ACC, total_RNA, total_TFs])\n",
        "    # return np.median(losses), np.median([total_ACC, total_RNA, total_TFs])\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args, lr_scheduler):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    # lr = args.w_lr * (0.1 ** (epoch // 30))   # option 1\n",
        "    lr_scheduler.step()     # option 2\n",
        "    lr = lr_scheduler.get_lr()[0]\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    args = config()\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(args.seed)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    DataPath = args.DataDir\n",
        "    logger = get_logger(os.path.join(os.path.join(DataPath, args.name, 'staticConvNet'), \"{}.log\".format(args.name)))\n",
        "\n",
        "    trainDataset = LoadDataset(args, dataPath=DataPath, dataFile=args.TrainingDataFile,\n",
        "                               labelFile=args.TrainingLabelFile)\n",
        "    # define sampler\n",
        "    train_sampler = torch.utils.data.sampler.RandomSampler(trainDataset)\n",
        "\n",
        "    trainLoader = tdata.DataLoader(trainDataset, batch_size=args.BATCH_SIZE, shuffle=(train_sampler is None),\n",
        "                                   num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "    # Load the testing dataset, and create a data loader to generate a batch. CHANGE THIS ONCE TESTING DATASET IS READY\n",
        "    valDataset = LoadDataset(args, dataPath=DataPath, dataFile=args.TestingDataFile, labelFile=args.TestingLabelFile)\n",
        "    valLoader = tdata.DataLoader(dataset=valDataset, batch_size=args.BATCH_SIZE, shuffle=False,\n",
        "                                 num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "    # build the model and criterion\n",
        "    # model = BuildModel(args).to(device)\n",
        "    # model = DeepSEA(args.SEQ_LEN, args.NUM_OUTPUTS).to(device)\n",
        "    model = DeeperDeepSEA(args.SEQ_LEN, args.NUM_OUTPUTS).to(device)\n",
        "    # model = get_model(load_weights=False)\n",
        "\n",
        "    # Add a sigmoid activation function to the output.  Use a binary cross entropy\n",
        "    criterion = nn.BCELoss().to(device)\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # optimiser = topti.Adam(model.parameters(), lr=args.w_lr)  # Minimise the loss using the Adam algorithm.\n",
        "    optimiser = torch.optim.Adam(model.parameters(), args.w_lr, betas=(0.5, 0.999),\n",
        "                                   weight_decay=args.w_weight_decay)\n",
        "    # print(model.parameters())\n",
        "    # optimiser = torch.optim.SGD(model.parameters(), args.w_lr, momentum=args.w_momentum, weight_decay=args.w_weight_decay)\n",
        "\n",
        "    # model = DeepSEA(args.SEQ_LEN, args.NUM_OUTPUTS).to(device)\n",
        "    # criterion = nn.BCELoss()\n",
        "    # optimiser = (torch.optim.SGD, {\"lr\": args.w_lr, \"weight_decay\": 1e-6, \"momentum\": 0.9})\n",
        "\n",
        "\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, args.nEpochs, eta_min=args.w_lr_min)\n",
        "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, args.nEpochs, eta_min=0)\n",
        "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min')\n",
        "\n",
        "    best_acc1 = 0.0\n",
        "    for epoch in range(args.start_epoch, args.nEpochs):\n",
        "\n",
        "        adjust_learning_rate(optimiser, epoch, args, lr_scheduler)\n",
        "\n",
        "        # train for one epoch\n",
        "        train(trainLoader, model, criterion, optimiser, epoch, args, logger, device)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        acc1 = validate(valLoader, model, criterion, args, logger, device)\n",
        "\n",
        "        # scheduler.step(acc1[1])\n",
        "\n",
        "        # remember best acc@1 and save checkpoint\n",
        "        is_best = acc1 > best_acc1\n",
        "        best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "    # if (args.save_model):\n",
        "    #     torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "    torch.save(model.state_dict(), \"deepBrain2_cnn.pt\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX-riYdnp9Pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}